diff --git a/cinder/templates/bin/_ceph-keyring.sh.tpl b/cinder/templates/bin/_ceph-keyring.sh.tpl
index 521a7484..6062c471 100644
--- a/cinder/templates/bin/_ceph-keyring.sh.tpl
+++ b/cinder/templates/bin/_ceph-keyring.sh.tpl
@@ -29,4 +29,11 @@ cat <<EOF > /etc/ceph/ceph.client.${EXTERNAL_RBD_USER}.keyring
 EOF
 {{- end }}
 
+# Combine base ceph.conf with EC pool user overrides
+cp /tmp/ceph-base.conf /etc/ceph/ceph.conf
+if [ -f /tmp/ceph-ec-override.conf ]; then
+  echo "" >> /etc/ceph/ceph.conf
+  cat /tmp/ceph-ec-override.conf >> /etc/ceph/ceph.conf
+fi
+
 exit 0
diff --git a/cinder/templates/bin/_storage-init.sh.tpl b/cinder/templates/bin/_storage-init.sh.tpl
index 4f945e2c..66127c38 100644
--- a/cinder/templates/bin/_storage-init.sh.tpl
+++ b/cinder/templates/bin/_storage-init.sh.tpl
@@ -38,20 +38,83 @@ if [ "x$STORAGE_BACKEND" == "xcinder.volume.drivers.rbd.RBDDriver" ]; then
     ceph osd pool set $1 nosizechange ${size_protection}
     ceph osd pool set $1 crush_rule "${RBD_POOL_CRUSH_RULE}"
   }
-  ensure_pool ${RBD_POOL_NAME} ${RBD_POOL_CHUNK_SIZE} ${RBD_POOL_APP_NAME}
+
+  function ensure_ec_profile () {
+    local name=$1 k=$2 m=$3 device_class=$4 failure_domain=$5
+    if ceph osd erasure-code-profile get $name 2>/dev/null; then
+      echo "EC profile $name already exists"
+    else
+      ceph osd erasure-code-profile set $name \
+        k=$k m=$m \
+        crush-device-class=$device_class \
+        crush-failure-domain=$failure_domain
+    fi
+  }
+
+  function ensure_ec_data_pool () {
+    local pool_name=$1 profile=$2 app_name=$3
+    if ceph osd pool stats $pool_name 2>/dev/null; then
+      echo "EC data pool $pool_name already exists"
+    else
+      # Ceph auto-creates CRUSH rule from EC profile settings
+      ceph osd pool create $pool_name erasure $profile
+    fi
+
+    if [[ $(ceph mgr versions | awk '/version/{print $3}' | cut -d. -f1) -ge 12 ]]; then
+      ceph osd pool application enable $pool_name $app_name
+    fi
+
+    ceph osd pool set $pool_name allow_ec_overwrites true
+  }
+
+  function ensure_ec_metadata_pool () {
+    local pool_name=$1 chunk_size=$2 app_name=$3 replication=$4 crush_rule=$5
+
+    ceph osd pool stats $pool_name || ceph osd pool create $pool_name $chunk_size
+
+    if [[ $(ceph mgr versions | awk '/version/{print $3}' | cut -d. -f1) -ge 12 ]]; then
+      ceph osd pool application enable $pool_name $app_name
+    fi
+
+    size_protection=$(ceph osd pool get $pool_name nosizechange | cut -f2 -d: | tr -d '[:space:]')
+    ceph osd pool set $pool_name nosizechange 0
+    ceph osd pool set $pool_name size $replication --yes-i-really-mean-it
+    ceph osd pool set $pool_name nosizechange ${size_protection}
+    ceph osd pool set $pool_name crush_rule "$crush_rule"
+  }
+
+  if [ "x$EC_ENABLED" == "xtrue" ]; then
+    ensure_ec_profile "${EC_PROFILE_NAME}" "${EC_PROFILE_K}" "${EC_PROFILE_M}" \
+      "${EC_PROFILE_DEVICE_CLASS}" "${EC_PROFILE_FAILURE_DOMAIN}"
+
+    ensure_ec_data_pool "${EC_DATA_POOL_NAME}" "${EC_PROFILE_NAME}" "${EC_POOL_APP_NAME}"
+
+    ensure_ec_metadata_pool "${RBD_POOL_NAME}" "${EC_METADATA_POOL_CHUNK_SIZE}" \
+      "${EC_POOL_APP_NAME}" "${EC_METADATA_POOL_REPLICATION}" "${EC_METADATA_POOL_CRUSH_RULE}"
+  else
+    ensure_pool ${RBD_POOL_NAME} ${RBD_POOL_CHUNK_SIZE} ${RBD_POOL_APP_NAME}
+  fi
+
+  # Set OSD caps based on whether EC is enabled
+  # EC backends need access to both metadata pool and data pool
+  if [ "x$EC_ENABLED" == "xtrue" ]; then
+    OSD_CAPS="profile rbd pool=${RBD_POOL_NAME}, profile rbd pool=${EC_DATA_POOL_NAME}"
+  else
+    OSD_CAPS="profile rbd"
+  fi
 
   if USERINFO=$(ceph auth get client.${RBD_POOL_USER}); then
     echo "Cephx user client.${RBD_POOL_USER} already exist."
     echo "Update its cephx caps"
     ceph auth caps client.${RBD_POOL_USER} \
       mon "profile rbd" \
-      osd "profile rbd"
+      osd "${OSD_CAPS}"
     ceph auth get client.${RBD_POOL_USER} -o ${KEYRING}
   else
     #NOTE(JCL): Restrict Cinder permissions to what is needed. MON Read only and RBD access to Cinder pool only.
     ceph auth get-or-create client.${RBD_POOL_USER} \
       mon "profile rbd" \
-      osd "profile rbd" \
+      osd "${OSD_CAPS}" \
       -o ${KEYRING}
   fi
 
diff --git a/cinder/templates/configmap-etc.yaml b/cinder/templates/configmap-etc.yaml
index 97bfe0f7..93b5e898 100644
--- a/cinder/templates/configmap-etc.yaml
+++ b/cinder/templates/configmap-etc.yaml
@@ -205,6 +205,21 @@ data:
 {{- if and .Values.ceph_client.enable_external_ceph_backend (not .Values.ceph_client.external_ceph.configmap) }}
   external-ceph.conf: {{ include "helm-toolkit.utils.to_oslo_conf" .Values.ceph_client.external_ceph.conf | b64enc }}
 {{- end }}
+{{- /* Generate ceph.conf sections for EC pool users */ -}}
+{{- $ec_ceph_conf := dict -}}
+{{- range $name, $backend := .Values.conf.backends -}}
+{{- if and $backend.rbd_pool $backend.rbd_user -}}
+{{- $ec_pool := index $.Values.conf.ceph.ec_pools $backend.rbd_pool -}}
+{{- if $ec_pool -}}
+{{- $client_section := printf "client.%s" $backend.rbd_user -}}
+{{- $data_pool_name := printf "%s.data" $backend.rbd_pool -}}
+{{- $_ := set $ec_ceph_conf $client_section (dict "rbd default data pool" $data_pool_name) -}}
+{{- end -}}
+{{- end -}}
+{{- end -}}
+{{- if $ec_ceph_conf }}
+  ceph-ec-override.conf: {{ include "helm-toolkit.utils.to_ini" $ec_ceph_conf | b64enc }}
+{{- end }}
 {{- end }}
 {{- range $key, $backend := .Values.conf.backends }}
 {{- if $backend.nfs_shares_config }}
diff --git a/cinder/templates/deployment-backup.yaml b/cinder/templates/deployment-backup.yaml
index 83437af2..1830e0c5 100644
--- a/cinder/templates/deployment-backup.yaml
+++ b/cinder/templates/deployment-backup.yaml
@@ -90,6 +90,16 @@ spec:
               mountPath: /tmp/client-keyring
               subPath: key
               readOnly: true
+            - name: ceph-etc
+              mountPath: /tmp/ceph-base.conf
+              subPath: ceph.conf
+              readOnly: true
+            {{- if $envAll.Values.conf.ceph.ec_pools }}
+            - name: cinder-etc
+              mountPath: /tmp/ceph-ec-override.conf
+              subPath: ceph-ec-override.conf
+              readOnly: true
+            {{- end }}
         {{ end }}
         {{- range $name := rest (splitList "," (include "cinder.utils.ceph_backend_list" $envAll)) }}
           {{- $backend := index $envAll.Values.conf.backends $name }}
@@ -119,6 +129,16 @@ spec:
               mountPath: /tmp/client-keyring
               subPath: key
               readOnly: true
+            - name: ceph-etc
+              mountPath: /tmp/ceph-base.conf
+              subPath: ceph.conf
+              readOnly: true
+            {{- if $envAll.Values.conf.ceph.ec_pools }}
+            - name: cinder-etc
+              mountPath: /tmp/ceph-ec-override.conf
+              subPath: ceph-ec-override.conf
+              readOnly: true
+            {{- end }}
            {{- if and $envAll.Values.ceph_client.enable_external_ceph_backend $envAll.Values.ceph_client.external_ceph.rbd_user }}
             - name: external-ceph-keyring
               mountPath: /tmp/external-ceph-client-keyring
@@ -187,12 +207,8 @@ spec:
             {{ if or (contains "cinder.backup.drivers.ceph" .Values.conf.cinder.DEFAULT.backup_driver) (eq "true" (include "cinder.utils.has_ceph_backend" $envAll)) }}
             - name: etcceph
               mountPath: /etc/ceph
-            {{- if not .Values.backup.external_ceph_rbd.enabled }}
-            - name: ceph-etc
-              mountPath: /etc/ceph/ceph.conf
-              subPath: ceph.conf
-              readOnly: true
-            {{- else if .Values.backup.external_ceph_rbd.configmap }}
+            {{- if .Values.backup.external_ceph_rbd.enabled }}
+            {{- if .Values.backup.external_ceph_rbd.configmap }}
             - name: external-backup-ceph-etc
               mountPath: /etc/ceph/ceph.conf
               subPath: ceph.conf
@@ -203,6 +219,7 @@ spec:
               subPath: external-backup-ceph.conf
               readOnly: true
             {{- end }}
+            {{- end }}
             {{- if (contains "cinder.backup.drivers.ceph" .Values.conf.cinder.DEFAULT.backup_driver) }}
             - name: ceph-backup-keyring
               mountPath: /tmp/client-keyring
diff --git a/cinder/templates/deployment-volume.yaml b/cinder/templates/deployment-volume.yaml
index 80eb8247..e2608867 100644
--- a/cinder/templates/deployment-volume.yaml
+++ b/cinder/templates/deployment-volume.yaml
@@ -96,6 +96,16 @@ spec:
               mountPath: /tmp/client-keyring
               subPath: key
               readOnly: true
+            - name: ceph-etc
+              mountPath: /tmp/ceph-base.conf
+              subPath: ceph.conf
+              readOnly: true
+            {{- if $envAll.Values.conf.ceph.ec_pools }}
+            - name: cinder-etc
+              mountPath: /tmp/ceph-ec-override.conf
+              subPath: ceph-ec-override.conf
+              readOnly: true
+            {{- end }}
             {{- if and $envAll.Values.ceph_client.enable_external_ceph_backend $envAll.Values.ceph_client.external_ceph.rbd_user }}
             - name: external-ceph-keyring
               mountPath: /tmp/external-ceph-client-keyring
@@ -187,10 +197,6 @@ spec:
             {{- if eq "true" (include "cinder.utils.has_ceph_backend" $envAll) }}
             - name: etcceph
               mountPath: /etc/ceph
-            - name: ceph-etc
-              mountPath: /etc/ceph/ceph.conf
-              subPath: ceph.conf
-              readOnly: true
             - name: ceph-keyring
               mountPath: /tmp/client-keyring
               subPath: key
diff --git a/cinder/templates/job-storage-init.yaml b/cinder/templates/job-storage-init.yaml
index 350b013a..e0cfd78d 100644
--- a/cinder/templates/job-storage-init.yaml
+++ b/cinder/templates/job-storage-init.yaml
@@ -101,8 +101,7 @@ spec:
       containers:
         {{- range $name, $backend := .Values.conf.backends }}
           {{- if (eq "true" ( dict "backend" $backend | include "cinder.utils.is_ceph_backend" )) }}
-            {{- if eq $internal_ceph_backend $name }}
-        - name: cinder-storage-init-{{ $name | lower }}
+        - name: cinder-storage-init-{{ $name | lower | replace "_" "-" }}
 {{ tuple $envAll "cinder_storage_init" | include "helm-toolkit.snippets.image" | indent 10 }}
 {{ tuple $envAll $envAll.Values.pod.resources.jobs.storage_init | include "helm-toolkit.snippets.kubernetes_resources" | indent 10 }}
           env:
@@ -114,18 +113,46 @@ spec:
               value: {{ $backend.volume_driver | quote }}
             - name: RBD_POOL_NAME
               value: {{ $backend.rbd_pool | quote }}
-            - name: RBD_POOL_APP_NAME
-              value: {{ (index $envAll.Values.conf.ceph.pools $backend.rbd_pool).app_name | quote }}
             - name: RBD_POOL_USER
               value: {{ $backend.rbd_user | quote }}
+            - name: RBD_POOL_SECRET
+              value: {{ $envAll.Values.secrets.rbd.volume | quote }}
+            {{- $ec_pool := index $envAll.Values.conf.ceph.ec_pools $backend.rbd_pool }}
+            {{- if $ec_pool }}
+            - name: EC_ENABLED
+              value: "true"
+            - name: EC_PROFILE_NAME
+              value: "{{ $backend.rbd_pool | replace "." "_" }}_profile"
+            - name: EC_PROFILE_K
+              value: {{ $ec_pool.data_pool.ec_profile.k | quote }}
+            - name: EC_PROFILE_M
+              value: {{ $ec_pool.data_pool.ec_profile.m | quote }}
+            - name: EC_PROFILE_DEVICE_CLASS
+              value: {{ $ec_pool.data_pool.ec_profile.crush_device_class | quote }}
+            - name: EC_PROFILE_FAILURE_DOMAIN
+              value: {{ $ec_pool.data_pool.ec_profile.crush_failure_domain | quote }}
+            - name: EC_DATA_POOL_NAME
+              value: "{{ $backend.rbd_pool }}.data"
+            - name: EC_METADATA_POOL_CHUNK_SIZE
+              value: {{ $ec_pool.metadata_pool.chunk_size | quote }}
+            - name: EC_METADATA_POOL_REPLICATION
+              value: {{ $ec_pool.metadata_pool.replication | quote }}
+            - name: EC_METADATA_POOL_CRUSH_RULE
+              value: {{ $ec_pool.metadata_pool.crush_rule | quote }}
+            - name: EC_POOL_APP_NAME
+              value: {{ $ec_pool.app_name | quote }}
+            {{- else }}
+            - name: EC_ENABLED
+              value: "false"
+            - name: RBD_POOL_APP_NAME
+              value: {{ (index $envAll.Values.conf.ceph.pools $backend.rbd_pool).app_name | quote }}
             - name: RBD_POOL_CRUSH_RULE
               value: {{ (index $envAll.Values.conf.ceph.pools $backend.rbd_pool).crush_rule | quote }}
             - name: RBD_POOL_REPLICATION
               value: {{ (index $envAll.Values.conf.ceph.pools $backend.rbd_pool).replication | quote }}
             - name: RBD_POOL_CHUNK_SIZE
               value: {{ (index $envAll.Values.conf.ceph.pools $backend.rbd_pool).chunk_size | quote }}
-            - name: RBD_POOL_SECRET
-              value: {{ $envAll.Values.secrets.rbd.volume | quote }}
+            {{- end }}
           command:
             - /tmp/storage-init.sh
           volumeMounts:
@@ -147,7 +174,6 @@ spec:
               subPath: key
               readOnly: true
             {{- end }}
-            {{- end }}
         {{- end }}
       {{- end }}
       volumes:
diff --git a/cinder/values.yaml b/cinder/values.yaml
index 6f1d32a3..41e004a1 100644
--- a/cinder/values.yaml
+++ b/cinder/values.yaml
@@ -787,6 +787,24 @@ conf:
         crush_rule: replicated_rule
         chunk_size: 8
         app_name: cinder-volume
+
+    # Erasure coded pools for Cinder backends with device class targeting.
+    # Ceph auto-creates CRUSH rule from the EC profile settings.
+    #
+    # ec_pools:
+    #   cinder.volumes.hdd:
+    #     metadata_pool:
+    #       replication: 3
+    #       crush_rule: replicated_hdd
+    #       chunk_size: 8
+    #     data_pool:
+    #       ec_profile:
+    #         k: 4
+    #         m: 2
+    #         crush_device_class: hdd
+    #         crush_failure_domain: host
+    #     app_name: cinder-volume
+    ec_pools: {}
   cinder:
     DEFAULT:
       volume_usage_audit_period: hour
@@ -943,6 +961,24 @@ conf:
       image_volume_cache_enabled: true
       image_volume_cache_max_size_gb: 200
       image_volume_cache_max_count: 50
+
+    # EC backend example for HDD tier with erasure coding.
+    #
+    # rbd_hdd:
+    #   volume_driver: cinder.volume.drivers.rbd.RBDDriver
+    #   volume_backend_name: rbd_hdd
+    #   rbd_pool: cinder.volumes.hdd
+    #   rbd_ceph_conf: "/etc/ceph/ceph.conf"
+    #   rbd_flatten_volume_from_snapshot: false
+    #   report_discard_supported: true
+    #   rbd_max_clone_depth: 5
+    #   rbd_store_chunk_size: 4
+    #   rados_connect_timeout: -1
+    #   rbd_user: cinder
+    #   rbd_secret_uuid: 457eb676-33da-42ec-9a8c-9293d545c337
+    #   image_volume_cache_enabled: true
+    #   image_volume_cache_max_size_gb: 200
+    #   image_volume_cache_max_count: 50
   rally_tests:
     run_tempest: false
     clean_up: |
