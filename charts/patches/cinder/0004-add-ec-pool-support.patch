diff --git a/cinder/templates/bin/_storage-init.sh.tpl b/cinder/templates/bin/_storage-init.sh.tpl
index 4f945e2c..66127c38 100644
--- a/cinder/templates/bin/_storage-init.sh.tpl
+++ b/cinder/templates/bin/_storage-init.sh.tpl
@@ -38,20 +38,83 @@ if [ "x$STORAGE_BACKEND" == "xcinder.volume.drivers.rbd.RBDDriver" ]; then
     ceph osd pool set $1 nosizechange ${size_protection}
     ceph osd pool set $1 crush_rule "${RBD_POOL_CRUSH_RULE}"
   }
-  ensure_pool ${RBD_POOL_NAME} ${RBD_POOL_CHUNK_SIZE} ${RBD_POOL_APP_NAME}
+
+  function ensure_ec_profile () {
+    local name=$1 k=$2 m=$3 device_class=$4 failure_domain=$5
+    if ceph osd erasure-code-profile get $name 2>/dev/null; then
+      echo "EC profile $name already exists"
+    else
+      ceph osd erasure-code-profile set $name \
+        k=$k m=$m \
+        crush-device-class=$device_class \
+        crush-failure-domain=$failure_domain
+    fi
+  }
+
+  function ensure_ec_data_pool () {
+    local pool_name=$1 profile=$2 app_name=$3
+    if ceph osd pool stats $pool_name 2>/dev/null; then
+      echo "EC data pool $pool_name already exists"
+    else
+      # Ceph auto-creates CRUSH rule from EC profile settings
+      ceph osd pool create $pool_name erasure $profile
+    fi
+
+    if [[ $(ceph mgr versions | awk '/version/{print $3}' | cut -d. -f1) -ge 12 ]]; then
+      ceph osd pool application enable $pool_name $app_name
+    fi
+
+    ceph osd pool set $pool_name allow_ec_overwrites true
+  }
+
+  function ensure_ec_metadata_pool () {
+    local pool_name=$1 chunk_size=$2 app_name=$3 replication=$4 crush_rule=$5
+
+    ceph osd pool stats $pool_name || ceph osd pool create $pool_name $chunk_size
+
+    if [[ $(ceph mgr versions | awk '/version/{print $3}' | cut -d. -f1) -ge 12 ]]; then
+      ceph osd pool application enable $pool_name $app_name
+    fi
+
+    size_protection=$(ceph osd pool get $pool_name nosizechange | cut -f2 -d: | tr -d '[:space:]')
+    ceph osd pool set $pool_name nosizechange 0
+    ceph osd pool set $pool_name size $replication --yes-i-really-mean-it
+    ceph osd pool set $pool_name nosizechange ${size_protection}
+    ceph osd pool set $pool_name crush_rule "$crush_rule"
+  }
+
+  if [ "x$EC_ENABLED" == "xtrue" ]; then
+    ensure_ec_profile "${EC_PROFILE_NAME}" "${EC_PROFILE_K}" "${EC_PROFILE_M}" \
+      "${EC_PROFILE_DEVICE_CLASS}" "${EC_PROFILE_FAILURE_DOMAIN}"
+
+    ensure_ec_data_pool "${EC_DATA_POOL_NAME}" "${EC_PROFILE_NAME}" "${EC_POOL_APP_NAME}"
+
+    ensure_ec_metadata_pool "${RBD_POOL_NAME}" "${EC_METADATA_POOL_CHUNK_SIZE}" \
+      "${EC_POOL_APP_NAME}" "${EC_METADATA_POOL_REPLICATION}" "${EC_METADATA_POOL_CRUSH_RULE}"
+  else
+    ensure_pool ${RBD_POOL_NAME} ${RBD_POOL_CHUNK_SIZE} ${RBD_POOL_APP_NAME}
+  fi
+
+  # Set OSD caps based on whether EC is enabled
+  # EC backends need access to both metadata pool and data pool
+  if [ "x$EC_ENABLED" == "xtrue" ]; then
+    OSD_CAPS="profile rbd pool=${RBD_POOL_NAME}, profile rbd pool=${EC_DATA_POOL_NAME}"
+  else
+    OSD_CAPS="profile rbd"
+  fi
 
   if USERINFO=$(ceph auth get client.${RBD_POOL_USER}); then
     echo "Cephx user client.${RBD_POOL_USER} already exist."
     echo "Update its cephx caps"
     ceph auth caps client.${RBD_POOL_USER} \
       mon "profile rbd" \
-      osd "profile rbd"
+      osd "${OSD_CAPS}"
     ceph auth get client.${RBD_POOL_USER} -o ${KEYRING}
   else
     #NOTE(JCL): Restrict Cinder permissions to what is needed. MON Read only and RBD access to Cinder pool only.
     ceph auth get-or-create client.${RBD_POOL_USER} \
       mon "profile rbd" \
-      osd "profile rbd" \
+      osd "${OSD_CAPS}" \
       -o ${KEYRING}
   fi
 
diff --git a/cinder/templates/job-storage-init.yaml b/cinder/templates/job-storage-init.yaml
index 350b013a..e0cfd78d 100644
--- a/cinder/templates/job-storage-init.yaml
+++ b/cinder/templates/job-storage-init.yaml
@@ -101,8 +101,7 @@ spec:
       containers:
         {{- range $name, $backend := .Values.conf.backends }}
           {{- if (eq "true" ( dict "backend" $backend | include "cinder.utils.is_ceph_backend" )) }}
-            {{- if eq $internal_ceph_backend $name }}
-        - name: cinder-storage-init-{{ $name | lower }}
+        - name: cinder-storage-init-{{ $name | lower | replace "_" "-" }}
 {{ tuple $envAll "cinder_storage_init" | include "helm-toolkit.snippets.image" | indent 10 }}
 {{ tuple $envAll $envAll.Values.pod.resources.jobs.storage_init | include "helm-toolkit.snippets.kubernetes_resources" | indent 10 }}
           env:
@@ -114,18 +113,46 @@ spec:
               value: {{ $backend.volume_driver | quote }}
             - name: RBD_POOL_NAME
               value: {{ $backend.rbd_pool | quote }}
-            - name: RBD_POOL_APP_NAME
-              value: {{ (index $envAll.Values.conf.ceph.pools $backend.rbd_pool).app_name | quote }}
             - name: RBD_POOL_USER
               value: {{ $backend.rbd_user | quote }}
+            - name: RBD_POOL_SECRET
+              value: {{ $envAll.Values.secrets.rbd.volume | quote }}
+            {{- $ec_pool := index $envAll.Values.conf.ceph.ec_pools $backend.rbd_pool }}
+            {{- if $ec_pool }}
+            - name: EC_ENABLED
+              value: "true"
+            - name: EC_PROFILE_NAME
+              value: "{{ $backend.rbd_pool | replace "." "_" }}_profile"
+            - name: EC_PROFILE_K
+              value: {{ $ec_pool.data_pool.ec_profile.k | quote }}
+            - name: EC_PROFILE_M
+              value: {{ $ec_pool.data_pool.ec_profile.m | quote }}
+            - name: EC_PROFILE_DEVICE_CLASS
+              value: {{ $ec_pool.data_pool.ec_profile.crush_device_class | quote }}
+            - name: EC_PROFILE_FAILURE_DOMAIN
+              value: {{ $ec_pool.data_pool.ec_profile.crush_failure_domain | quote }}
+            - name: EC_DATA_POOL_NAME
+              value: "{{ $backend.rbd_pool }}.data"
+            - name: EC_METADATA_POOL_CHUNK_SIZE
+              value: {{ $ec_pool.metadata_pool.chunk_size | quote }}
+            - name: EC_METADATA_POOL_REPLICATION
+              value: {{ $ec_pool.metadata_pool.replication | quote }}
+            - name: EC_METADATA_POOL_CRUSH_RULE
+              value: {{ $ec_pool.metadata_pool.crush_rule | quote }}
+            - name: EC_POOL_APP_NAME
+              value: {{ $ec_pool.app_name | quote }}
+            {{- else }}
+            - name: EC_ENABLED
+              value: "false"
+            - name: RBD_POOL_APP_NAME
+              value: {{ (index $envAll.Values.conf.ceph.pools $backend.rbd_pool).app_name | quote }}
             - name: RBD_POOL_CRUSH_RULE
               value: {{ (index $envAll.Values.conf.ceph.pools $backend.rbd_pool).crush_rule | quote }}
             - name: RBD_POOL_REPLICATION
               value: {{ (index $envAll.Values.conf.ceph.pools $backend.rbd_pool).replication | quote }}
             - name: RBD_POOL_CHUNK_SIZE
               value: {{ (index $envAll.Values.conf.ceph.pools $backend.rbd_pool).chunk_size | quote }}
-            - name: RBD_POOL_SECRET
-              value: {{ $envAll.Values.secrets.rbd.volume | quote }}
+            {{- end }}
           command:
             - /tmp/storage-init.sh
           volumeMounts:
@@ -147,7 +174,6 @@ spec:
               subPath: key
               readOnly: true
             {{- end }}
-            {{- end }}
         {{- end }}
       {{- end }}
       volumes:
diff --git a/cinder/values.yaml b/cinder/values.yaml
index 6f1d32a3..41e004a1 100644
--- a/cinder/values.yaml
+++ b/cinder/values.yaml
@@ -787,6 +787,24 @@ conf:
         crush_rule: replicated_rule
         chunk_size: 8
         app_name: cinder-volume
+
+    # Erasure coded pools for Cinder backends with device class targeting.
+    # Ceph auto-creates CRUSH rule from the EC profile settings.
+    #
+    # ec_pools:
+    #   cinder.volumes.hdd:
+    #     metadata_pool:
+    #       replication: 3
+    #       crush_rule: replicated_hdd
+    #       chunk_size: 8
+    #     data_pool:
+    #       ec_profile:
+    #         k: 4
+    #         m: 2
+    #         crush_device_class: hdd
+    #         crush_failure_domain: host
+    #     app_name: cinder-volume
+    ec_pools: {}
   cinder:
     DEFAULT:
       volume_usage_audit_period: hour
@@ -943,6 +961,24 @@ conf:
       image_volume_cache_enabled: true
       image_volume_cache_max_size_gb: 200
       image_volume_cache_max_count: 50
+
+    # EC backend example for HDD tier with erasure coding.
+    #
+    # rbd_hdd:
+    #   volume_driver: cinder.volume.drivers.rbd.RBDDriver
+    #   volume_backend_name: rbd_hdd
+    #   rbd_pool: cinder.volumes.hdd
+    #   rbd_ceph_conf: "/etc/ceph/ceph.conf"
+    #   rbd_flatten_volume_from_snapshot: false
+    #   report_discard_supported: true
+    #   rbd_max_clone_depth: 5
+    #   rbd_store_chunk_size: 4
+    #   rados_connect_timeout: -1
+    #   rbd_user: cinder
+    #   rbd_secret_uuid: 457eb676-33da-42ec-9a8c-9293d545c337
+    #   image_volume_cache_enabled: true
+    #   image_volume_cache_max_size_gb: 200
+    #   image_volume_cache_max_count: 50
   rally_tests:
     run_tempest: false
     clean_up: |
