diff --git a/cinder/templates/bin/_storage-init.sh.tpl b/cinder/templates/bin/_storage-init.sh.tpl
index 4f945e2c..66127c38 100644
--- a/cinder/templates/bin/_storage-init.sh.tpl
+++ b/cinder/templates/bin/_storage-init.sh.tpl
@@ -38,20 +38,83 @@ if [ "x$STORAGE_BACKEND" == "xcinder.volume.drivers.rbd.RBDDriver" ]; then
     ceph osd pool set $1 nosizechange ${size_protection}
     ceph osd pool set $1 crush_rule "${RBD_POOL_CRUSH_RULE}"
   }
-  ensure_pool ${RBD_POOL_NAME} ${RBD_POOL_CHUNK_SIZE} ${RBD_POOL_APP_NAME}
+
+  function ensure_ec_profile () {
+    local name=$1 k=$2 m=$3 device_class=$4 failure_domain=$5
+    if ceph osd erasure-code-profile get $name 2>/dev/null; then
+      echo "EC profile $name already exists"
+    else
+      ceph osd erasure-code-profile set $name \
+        k=$k m=$m \
+        crush-device-class=$device_class \
+        crush-failure-domain=$failure_domain
+    fi
+  }
+
+  function ensure_ec_data_pool () {
+    local pool_name=$1 profile=$2 app_name=$3
+    if ceph osd pool stats $pool_name 2>/dev/null; then
+      echo "EC data pool $pool_name already exists"
+    else
+      # Ceph auto-creates CRUSH rule from EC profile settings
+      ceph osd pool create $pool_name erasure $profile
+    fi
+
+    if [[ $(ceph mgr versions | awk '/version/{print $3}' | cut -d. -f1) -ge 12 ]]; then
+      ceph osd pool application enable $pool_name $app_name
+    fi
+
+    ceph osd pool set $pool_name allow_ec_overwrites true
+  }
+
+  function ensure_ec_metadata_pool () {
+    local pool_name=$1 chunk_size=$2 app_name=$3 replication=$4 crush_rule=$5
+
+    ceph osd pool stats $pool_name || ceph osd pool create $pool_name $chunk_size
+
+    if [[ $(ceph mgr versions | awk '/version/{print $3}' | cut -d. -f1) -ge 12 ]]; then
+      ceph osd pool application enable $pool_name $app_name
+    fi
+
+    size_protection=$(ceph osd pool get $pool_name nosizechange | cut -f2 -d: | tr -d '[:space:]')
+    ceph osd pool set $pool_name nosizechange 0
+    ceph osd pool set $pool_name size $replication --yes-i-really-mean-it
+    ceph osd pool set $pool_name nosizechange ${size_protection}
+    ceph osd pool set $pool_name crush_rule "$crush_rule"
+  }
+
+  if [ "x$EC_ENABLED" == "xtrue" ]; then
+    ensure_ec_profile "${EC_PROFILE_NAME}" "${EC_PROFILE_K}" "${EC_PROFILE_M}" \
+      "${EC_PROFILE_DEVICE_CLASS}" "${EC_PROFILE_FAILURE_DOMAIN}"
+
+    ensure_ec_data_pool "${EC_DATA_POOL_NAME}" "${EC_PROFILE_NAME}" "${EC_POOL_APP_NAME}"
+
+    ensure_ec_metadata_pool "${RBD_POOL_NAME}" "${EC_METADATA_POOL_CHUNK_SIZE}" \
+      "${EC_POOL_APP_NAME}" "${EC_METADATA_POOL_REPLICATION}" "${EC_METADATA_POOL_CRUSH_RULE}"
+  else
+    ensure_pool ${RBD_POOL_NAME} ${RBD_POOL_CHUNK_SIZE} ${RBD_POOL_APP_NAME}
+  fi
+
+  # Set OSD caps based on whether EC is enabled
+  # EC backends need access to both metadata pool and data pool
+  if [ "x$EC_ENABLED" == "xtrue" ]; then
+    OSD_CAPS="profile rbd pool=${RBD_POOL_NAME}, profile rbd pool=${EC_DATA_POOL_NAME}"
+  else
+    OSD_CAPS="profile rbd"
+  fi

   if USERINFO=$(ceph auth get client.${RBD_POOL_USER}); then
     echo "Cephx user client.${RBD_POOL_USER} already exist."
     echo "Update its cephx caps"
     ceph auth caps client.${RBD_POOL_USER} \
       mon "profile rbd" \
-      osd "profile rbd"
+      osd "${OSD_CAPS}"
     ceph auth get client.${RBD_POOL_USER} -o ${KEYRING}
   else
     #NOTE(JCL): Restrict Cinder permissions to what is needed. MON Read only and RBD access to Cinder pool only.
     ceph auth get-or-create client.${RBD_POOL_USER} \
       mon "profile rbd" \
-      osd "profile rbd" \
+      osd "${OSD_CAPS}" \
       -o ${KEYRING}
   fi

diff --git a/cinder/templates/deployment-backup.yaml b/cinder/templates/deployment-backup.yaml
index 83437af2..aced3688 100644
--- a/cinder/templates/deployment-backup.yaml
+++ b/cinder/templates/deployment-backup.yaml
@@ -93,8 +93,7 @@ spec:
         {{ end }}
         {{- range $name := rest (splitList "," (include "cinder.utils.ceph_backend_list" $envAll)) }}
           {{- $backend := index $envAll.Values.conf.backends $name }}
-            {{- if eq $internal_ceph_backend $name }}
-        - name: ceph-keyring-placement-{{ $name | lower }}
+        - name: ceph-keyring-placement-{{ $name | lower | replace "_" "-" }}
 {{ tuple $envAll "cinder_backup" | include "helm-toolkit.snippets.image" | indent 10 }}
 {{ dict "envAll" $envAll "application" "cinder_backup" "container" "ceph_keyring_placement" | include "helm-toolkit.snippets.kubernetes_container_security_context" | indent 10 }}
           command:
@@ -115,17 +114,23 @@ spec:
               mountPath: /tmp/ceph-keyring.sh
               subPath: ceph-keyring.sh
               readOnly: true
+            {{- if eq $internal_ceph_backend $name }}
             - name: ceph-keyring
               mountPath: /tmp/client-keyring
               subPath: key
               readOnly: true
-           {{- if and $envAll.Values.ceph_client.enable_external_ceph_backend $envAll.Values.ceph_client.external_ceph.rbd_user }}
+            {{- else }}
+            - name: ceph-keyring-{{ $name | lower | replace "_" "-" }}
+              mountPath: /tmp/client-keyring
+              subPath: key
+              readOnly: true
+            {{- end }}
+            {{- if and $envAll.Values.ceph_client.enable_external_ceph_backend $envAll.Values.ceph_client.external_ceph.rbd_user }}
             - name: external-ceph-keyring
               mountPath: /tmp/external-ceph-client-keyring
               subPath: key
               readOnly: true
             {{- end }}
-            {{- end }}
         {{- end }}
         {{- if (contains "cinder.backup.drivers.posix" .Values.conf.cinder.DEFAULT.backup_driver) }}
         - name: ceph-backup-volume-perms
@@ -330,6 +335,13 @@ spec:
         - name: ceph-keyring
           secret:
             secretName: {{ .Values.secrets.rbd.volume | quote }}
+        {{- range $name := rest (splitList "," (include "cinder.utils.ceph_backend_list" $envAll)) }}
+        {{- if ne $internal_ceph_backend $name }}
+        - name: ceph-keyring-{{ $name | lower | replace "_" "-" }}
+          secret:
+            secretName: "{{ $envAll.Values.secrets.rbd.volume }}-{{ $name | lower | replace "_" "-" }}"
+        {{- end }}
+        {{- end }}
         {{- if and .Values.ceph_client.enable_external_ceph_backend .Values.ceph_client.external_ceph.rbd_user }}
         - name: external-ceph-keyring
           secret:
diff --git a/cinder/templates/deployment-volume.yaml b/cinder/templates/deployment-volume.yaml
index 80eb8247..75572ef7 100644
--- a/cinder/templates/deployment-volume.yaml
+++ b/cinder/templates/deployment-volume.yaml
@@ -70,8 +70,7 @@ spec:
 {{ tuple $envAll "volume" $mounts_cinder_volume_init | include "helm-toolkit.snippets.kubernetes_entrypoint_init_container" | indent 8 }}
         {{- range $name := rest (splitList "," (include "cinder.utils.ceph_backend_list" $envAll)) }}
           {{- $backend := index $envAll.Values.conf.backends $name }}
-            {{- if eq $internal_ceph_backend $name }}
-        - name: ceph-keyring-placement-{{ $name | lower }}
+        - name: ceph-keyring-placement-{{ $name | lower | replace "_" "-" }}
 {{ tuple $envAll "cinder_volume" | include "helm-toolkit.snippets.image" | indent 10 }}
 {{ dict "envAll" $envAll "application" "cinder_volume" "container" "ceph_keyring_placement" | include "helm-toolkit.snippets.kubernetes_container_security_context" | indent 10 }}
           command:
@@ -92,17 +91,23 @@ spec:
               mountPath: /tmp/ceph-keyring.sh
               subPath: ceph-keyring.sh
               readOnly: true
+            {{- if eq $internal_ceph_backend $name }}
             - name: ceph-keyring
               mountPath: /tmp/client-keyring
               subPath: key
               readOnly: true
+            {{- else }}
+            - name: ceph-keyring-{{ $name | lower | replace "_" "-" }}
+              mountPath: /tmp/client-keyring
+              subPath: key
+              readOnly: true
+            {{- end }}
             {{- if and $envAll.Values.ceph_client.enable_external_ceph_backend $envAll.Values.ceph_client.external_ceph.rbd_user }}
             - name: external-ceph-keyring
               mountPath: /tmp/external-ceph-client-keyring
               subPath: key
               readOnly: true
             {{- end }}
-            {{- end }}
         {{- end }}
         {{- if eq ( split "://" .Values.conf.cinder.coordination.backend_url )._0 "file" }}
         - name: ceph-coordination-volume-perms
@@ -327,6 +332,13 @@ spec:
         - name: ceph-keyring
           secret:
             secretName: {{ .Values.secrets.rbd.volume | quote }}
+        {{- range $name := rest (splitList "," (include "cinder.utils.ceph_backend_list" $envAll)) }}
+        {{- if ne $internal_ceph_backend $name }}
+        - name: ceph-keyring-{{ $name | lower | replace "_" "-" }}
+          secret:
+            secretName: "{{ $envAll.Values.secrets.rbd.volume }}-{{ $name | lower | replace "_" "-" }}"
+        {{- end }}
+        {{- end }}
         {{- if and .Values.ceph_client.enable_external_ceph_backend .Values.ceph_client.external_ceph.rbd_user }}
         - name: external-ceph-keyring
           secret:
diff --git a/cinder/templates/job-storage-init.yaml b/cinder/templates/job-storage-init.yaml
index 350b013a..51203f77 100644
--- a/cinder/templates/job-storage-init.yaml
+++ b/cinder/templates/job-storage-init.yaml
@@ -101,8 +101,7 @@ spec:
       containers:
         {{- range $name, $backend := .Values.conf.backends }}
           {{- if (eq "true" ( dict "backend" $backend | include "cinder.utils.is_ceph_backend" )) }}
-            {{- if eq $internal_ceph_backend $name }}
-        - name: cinder-storage-init-{{ $name | lower }}
+        - name: cinder-storage-init-{{ $name | lower | replace "_" "-" }}
 {{ tuple $envAll "cinder_storage_init" | include "helm-toolkit.snippets.image" | indent 10 }}
 {{ tuple $envAll $envAll.Values.pod.resources.jobs.storage_init | include "helm-toolkit.snippets.kubernetes_resources" | indent 10 }}
           env:
@@ -114,18 +113,51 @@ spec:
               value: {{ $backend.volume_driver | quote }}
             - name: RBD_POOL_NAME
               value: {{ $backend.rbd_pool | quote }}
-            - name: RBD_POOL_APP_NAME
-              value: {{ (index $envAll.Values.conf.ceph.pools $backend.rbd_pool).app_name | quote }}
             - name: RBD_POOL_USER
               value: {{ $backend.rbd_user | quote }}
+            {{- if eq $internal_ceph_backend $name }}
+            - name: RBD_POOL_SECRET
+              value: {{ $envAll.Values.secrets.rbd.volume | quote }}
+            {{- else }}
+            - name: RBD_POOL_SECRET
+              value: "{{ $envAll.Values.secrets.rbd.volume }}-{{ $name | lower | replace "_" "-" }}"
+            {{- end }}
+            {{- $ec_pool := index $envAll.Values.conf.ceph.ec_pools $backend.rbd_pool }}
+            {{- if $ec_pool }}
+            - name: EC_ENABLED
+              value: "true"
+            - name: EC_PROFILE_NAME
+              value: "{{ $backend.rbd_pool | replace "." "_" }}_profile"
+            - name: EC_PROFILE_K
+              value: {{ $ec_pool.data_pool.ec_profile.k | quote }}
+            - name: EC_PROFILE_M
+              value: {{ $ec_pool.data_pool.ec_profile.m | quote }}
+            - name: EC_PROFILE_DEVICE_CLASS
+              value: {{ $ec_pool.data_pool.ec_profile.crush_device_class | quote }}
+            - name: EC_PROFILE_FAILURE_DOMAIN
+              value: {{ $ec_pool.data_pool.ec_profile.crush_failure_domain | quote }}
+            - name: EC_DATA_POOL_NAME
+              value: "{{ $backend.rbd_pool }}.data"
+            - name: EC_METADATA_POOL_CHUNK_SIZE
+              value: {{ $ec_pool.metadata_pool.chunk_size | quote }}
+            - name: EC_METADATA_POOL_REPLICATION
+              value: {{ $ec_pool.metadata_pool.replication | quote }}
+            - name: EC_METADATA_POOL_CRUSH_RULE
+              value: {{ $ec_pool.metadata_pool.crush_rule | quote }}
+            - name: EC_POOL_APP_NAME
+              value: {{ $ec_pool.app_name | quote }}
+            {{- else }}
+            - name: EC_ENABLED
+              value: "false"
+            - name: RBD_POOL_APP_NAME
+              value: {{ (index $envAll.Values.conf.ceph.pools $backend.rbd_pool).app_name | quote }}
             - name: RBD_POOL_CRUSH_RULE
               value: {{ (index $envAll.Values.conf.ceph.pools $backend.rbd_pool).crush_rule | quote }}
             - name: RBD_POOL_REPLICATION
               value: {{ (index $envAll.Values.conf.ceph.pools $backend.rbd_pool).replication | quote }}
             - name: RBD_POOL_CHUNK_SIZE
               value: {{ (index $envAll.Values.conf.ceph.pools $backend.rbd_pool).chunk_size | quote }}
-            - name: RBD_POOL_SECRET
-              value: {{ $envAll.Values.secrets.rbd.volume | quote }}
+            {{- end }}
           command:
             - /tmp/storage-init.sh
           volumeMounts:
@@ -147,7 +179,6 @@ spec:
               subPath: key
               readOnly: true
             {{- end }}
-            {{- end }}
         {{- end }}
       {{- end }}
       volumes:
diff --git a/cinder/values.yaml b/cinder/values.yaml
index 6f1d32a3..41e004a1 100644
--- a/cinder/values.yaml
+++ b/cinder/values.yaml
@@ -787,6 +787,24 @@ conf:
         crush_rule: replicated_rule
         chunk_size: 8
         app_name: cinder-volume
+
+    # Erasure coded pools for Cinder backends with device class targeting.
+    # Ceph auto-creates CRUSH rule from the EC profile settings.
+    #
+    # ec_pools:
+    #   cinder.volumes.hdd:
+    #     metadata_pool:
+    #       replication: 3
+    #       crush_rule: replicated_hdd
+    #       chunk_size: 8
+    #     data_pool:
+    #       ec_profile:
+    #         k: 4
+    #         m: 2
+    #         crush_device_class: hdd
+    #         crush_failure_domain: host
+    #     app_name: cinder-volume
+    ec_pools: {}
   cinder:
     DEFAULT:
       volume_usage_audit_period: hour
@@ -943,6 +961,24 @@ conf:
       image_volume_cache_enabled: true
       image_volume_cache_max_size_gb: 200
       image_volume_cache_max_count: 50
+
+    # EC backend example for HDD tier with erasure coding.
+    #
+    # rbd_hdd:
+    #   volume_driver: cinder.volume.drivers.rbd.RBDDriver
+    #   volume_backend_name: rbd_hdd
+    #   rbd_pool: cinder.volumes.hdd
+    #   rbd_ceph_conf: "/etc/ceph/ceph.conf"
+    #   rbd_flatten_volume_from_snapshot: false
+    #   report_discard_supported: true
+    #   rbd_max_clone_depth: 5
+    #   rbd_store_chunk_size: 4
+    #   rados_connect_timeout: -1
+    #   rbd_user: cinder
+    #   rbd_secret_uuid: 457eb676-33da-42ec-9a8c-9293d545c337
+    #   image_volume_cache_enabled: true
+    #   image_volume_cache_max_size_gb: 200
+    #   image_volume_cache_max_count: 50
   rally_tests:
     run_tempest: false
     clean_up: |
